{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9071569,"sourceType":"datasetVersion","datasetId":5441139}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-07-31T07:07:50.401103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport os, pickle\nimport numpy as np\nimport evaluate\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import SegformerForSemanticSegmentation, SegformerConfig, SegformerImageProcessor\nfrom torchinfo import summary\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision import transforms\nfrom torch import nn\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom tqdm.notebook import tqdm\nfrom matplotlib.colors import ListedColormap, BoundaryNorm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b0\")\n\nconfig = base_model.config\nconfig.num_channels = 18\nconfig.num_labels = 2\nconfig.hidden_dropout_prob = 0.2\nconfig.classifier_dropout_prob = 0.2\nmodel = SegformerForSemanticSegmentation(config)\n\n# Added batch size to input_size\nsummary(model, input_size=(1, 18, 128, 128))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntest_input  = torch.randn(1, 18, 128, 128).to(device)\n\noutput = model(test_input)\n\nprint(output.logits.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\ndef get_indices(arr):\n    if arr.ndim != 3 or arr.shape[2] < 7:\n        raise ValueError(\"Input array must be 3-dimensional with at least 7 channels.\")\n    \n    bands = {\n        \"ndvi\": (arr[:, :, 4] - arr[:, :, 3]) / (arr[:, :, 4] + arr[:, :, 3] + 1e-7),\n        \"evi\": 2.5 * (arr[:, :, 4] - arr[:, :, 3]) / (arr[:, :, 4] + 6 * arr[:, :, 3] - 7.5 * arr[:, :, 1] + 1),\n        \"savi\": 1.5 * (arr[:, :, 4] - arr[:, :, 3]) / (arr[:, :, 4] + arr[:, :, 3] + 0.5),\n        \"msavi\": 0.5 * (2 * arr[:, :, 4] + 1 - np.sqrt((2 * arr[:, :, 4] + 1) ** 2 - 8 * (arr[:, :, 4] - arr[:, :, 3]))),\n        \"ndmi\": (arr[:, :, 4] - arr[:, :, 5]) / (arr[:, :, 4] + arr[:, :, 5] + 1e-7),\n        \"nbr\": (arr[:, :, 4] - arr[:, :, 6]) / (arr[:, :, 4] + arr[:, :, 6] + 1e-7),\n        \"nbr2\": (arr[:, :, 5] - arr[:, :, 6]) / (arr[:, :, 5] + arr[:, :, 6] + 1e-7),\n    }\n    for name in bands:\n        value = np.nan_to_num(bands[name])\n        arr = np.dstack((arr, value))\n    return arr\n\nclass SlidingWindowDataset(Dataset):\n    def __init__(self, pickle_dir, window_size=128, stride=64, reduce_indices=False):\n        self.pickle_dir = pickle_dir\n        self.window_size = window_size\n        self.stride = stride\n        self.reduce_indices = reduce_indices\n        self.processed_images, self.processed_masks = self._process_data()\n\n    def _process_data(self):\n        processed_images = []\n        processed_masks = []\n        \n        for file_name in os.listdir(self.pickle_dir):\n            if file_name.endswith('.pkl'):\n                with open(os.path.join(self.pickle_dir, file_name), 'rb') as f:\n                    img, mask = pickle.load(f, encoding='latin1')\n                \n                if img.ndim == 3 and img.shape[2] >= 7:\n                    img = get_indices(img)\n                    h, w, _ = img.shape\n                    for i in range(0, h - self.window_size + 1, self.stride):\n                        for j in range(0, w - self.window_size + 1, self.stride):\n                            window_img = img[i:i + self.window_size, j:j + self.window_size]\n                            window_mask = mask[i:i + self.window_size, j:j + self.window_size]\n                            class_0_ratio = np.sum(window_mask == 0) / window_mask.size\n                            class_1_ratio = np.sum(window_mask == 1) / window_mask.size\n                            class_2_ratio = np.sum(window_mask == 2) / window_mask.size\n                            if class_0_ratio < 0.5:\n                                if class_2_ratio > 0.4:\n                                    # Augment the image by rotating it 90 degrees 3 times\n                                    for _ in range(3):\n                                        window_img = np.rot90(window_img).copy()\n                                        window_mask = np.rot90(window_mask).copy()\n                                        processed_images.append(window_img)\n                                        processed_masks.append(window_mask)\n                                else:\n                                    processed_images.append(window_img)\n                                    processed_masks.append(window_mask)\n\n                else:\n                    print(f\"Skipping image with shape {img.shape} in file {file_name}\")\n        \n        return processed_images, processed_masks\n\n    def __len__(self):\n        return len(self.processed_images)\n\n    def __getitem__(self, idx):\n        image = self.processed_images[idx]\n        mask = self.processed_masks[idx]\n        \n        if mask.dtype == np.uint16:\n            mask = mask.astype(np.int64)\n            \n        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)  # Convert to CxHxW\n        mask = torch.tensor(mask, dtype=torch.long)\n        \n        if self.reduce_indices:\n            mask = mask - 1\n            mask[mask == -1] = 255\n\n        encoded_data = {\n            'pixel_values': image,\n            'labels': mask\n        }\n\n        return encoded_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_dir = '/kaggle/input/forestwatch-data/data/data/train/training_2015_pickled_data'\n\ndataset = SlidingWindowDataset(pickle_dir=root_dir, window_size=128, reduce_indices=True)\n\nSEED = 123\nLEARNING_RATE = 1e-5\nBATCH_SIZE = 16\nTRAIN_DEV_TEST_SPLIT = (0.8, 0.1, 0.1)\n\ngenerator = torch.Generator().manual_seed(SEED)\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, TRAIN_DEV_TEST_SPLIT, generator)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\nval_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)\ntest_dataloader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_dataloader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take a mask and check if it has any values other than 0 and 1\nmask = batch['labels']\nimage = batch['pixel_values']\n\nprint(torch.unique(mask))\nprint(mask.shape)\nprint(image.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Take a mask and check if it has any values other than 0 and 1\nmask = batch['labels']\n\nprint(torch.unique(mask))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(dataset))\n\n#3321","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Define optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n\n# Move model to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define the metric functions\ndef compute_accuracy_ignore_background(preds, labels, ignore_index=255):\n    preds = preds.cpu().numpy().flatten()\n    labels = labels.cpu().numpy().flatten()\n    mask = labels != ignore_index\n    preds = preds[mask]\n    labels = labels[mask]\n    accuracy = (preds == labels).mean()\n    return accuracy\n\ndef compute_miou_ignore_background(preds, labels, num_classes, ignore_index=255):\n    preds = preds.cpu().numpy().flatten()\n    labels = labels.cpu().numpy().flatten()\n    mask = labels != ignore_index\n    preds = preds[mask]\n    labels = labels[mask]\n    \n    ious = []\n    for cls in range(num_classes):\n        if cls == ignore_index:\n            continue\n        pred_inds = preds == cls\n        label_inds = labels == cls\n        intersection = np.logical_and(pred_inds, label_inds).sum()\n        union = np.logical_or(pred_inds, label_inds).sum()\n        if union == 0:\n            ious.append(float('nan'))\n        else:\n            ious.append(intersection / union)\n    return np.nanmean(ious)\n\ndef compute_precision_per_class_ignore_background(preds, labels, num_classes, ignore_index=255):\n    preds = preds.cpu().numpy().flatten()\n    labels = labels.cpu().numpy().flatten()\n    mask = labels != ignore_index\n    preds = preds[mask]\n    labels = labels[mask]\n    \n    precision_per_class = precision_score(labels, preds, average=None, labels=range(num_classes), zero_division=0)\n    return {f'class_{i}': precision for i, precision in enumerate(precision_per_class) if i != ignore_index}\n\ndef compute_recall_per_class_ignore_background(preds, labels, num_classes, ignore_index=255):\n    preds = preds.cpu().numpy().flatten()\n    labels = labels.cpu().numpy().flatten()\n    mask = labels != ignore_index\n    preds = preds[mask]\n    labels = labels[mask]\n    \n    recall_per_class = recall_score(labels, preds, average=None, labels=range(num_classes), zero_division=0)\n    return {f'class_{i}': recall for i, recall in enumerate(recall_per_class) if i != ignore_index}\n\ndef compute_f1_per_class_ignore_background(preds, labels, num_classes, ignore_index=255):\n    preds = preds.cpu().numpy().flatten()\n    labels = labels.cpu().numpy().flatten()\n    mask = labels != ignore_index\n    preds = preds[mask]\n    labels = labels[mask]\n    \n    f1_per_class = f1_score(labels, preds, average=None, labels=range(num_classes), zero_division=0)\n    return {f'class_{i}': f1 for i, f1 in enumerate(f1_per_class) if i != ignore_index}\n\n# Initialize the exponentially weighted average loss\newma_loss = None\nalpha = 0.1  # Smoothing factor\n\nmodel.train()\nfor epoch in range(20):  # loop over the dataset multiple times\n    losses = []\n    accuracies = []\n    mious = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n    print(\"Epoch:\", epoch)\n    t = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n    for idx, batch in t:\n        # Get the inputs\n        pixel_values = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward + backward + optimize\n        outputs = model(pixel_values=pixel_values, labels=labels)\n        loss, logits = outputs.loss, outputs.logits\n\n        loss.backward()\n        optimizer.step()\n\n        # Update the exponentially weighted average loss\n        if ewma_loss is None:\n            ewma_loss = loss.item()\n        else:\n            ewma_loss = alpha * loss.item() + (1 - alpha) * ewma_loss\n\n        # Evaluate\n        with torch.no_grad():\n            upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n            predicted = upsampled_logits.argmax(dim=1)\n\n            # Calculate metrics\n            accuracy = compute_accuracy_ignore_background(predicted, labels)\n            miou = compute_miou_ignore_background(predicted, labels, num_classes=2)\n            precision_per_class = compute_precision_per_class_ignore_background(predicted, labels, num_classes=2)\n            recall_per_class = compute_recall_per_class_ignore_background(predicted, labels, num_classes=2)\n            f1_per_class = compute_f1_per_class_ignore_background(predicted, labels, num_classes=2)\n\n        losses.append(loss.item())\n        accuracies.append(accuracy)\n        mious.append(miou)\n        precisions.append(precision_per_class)\n        recalls.append(recall_per_class)\n        f1_scores.append(f1_per_class)\n\n        # Print loss and metrics every batch\n        t.set_postfix(Loss=f\"{ewma_loss:.4f}\", Accuracy=f\"{accuracy:.4f}\", mIoU=f\"{miou:.4f}\")\n        t.update()\n\n    # Print loss and metrics every epoch\n    print(f\"Loss: {np.mean(losses):.4f}, Accuracy: {np.mean(accuracies):.4f}, mIoU: {np.nanmean(mious):.4f}\")\n    print(f\"Precision: {np.nanmean([list(p.values()) for p in precisions], axis=0)}\")\n    print(f\"Recall: {np.nanmean([list(r.values()) for r in recalls], axis=0)}\")\n    print(f\"F1 Score: {np.nanmean([list(f.values()) for f in f1_scores], axis=0)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Evaluate on the validation set\nmodel.eval()\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader):\n        pixel_values = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(pixel_values=pixel_values, labels=labels)\n        loss, logits = outputs.loss, outputs.logits\n        upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n        predicted = upsampled_logits.argmax(dim=1)\n\n        all_preds.append(predicted.detach().cpu())\n        all_labels.append(labels.detach().cpu())\n\nall_preds = torch.cat(all_preds)\nall_labels = torch.cat(all_labels)\n\n# Flatten the tensors\nall_preds_flat = all_preds.flatten()\nall_labels_flat = all_labels.flatten()\n\n# Mask to ignore the ignore_index\nignore_index = 255\nmask = all_labels_flat != ignore_index\n\n# Filter out the ignore_index\nfiltered_preds = all_preds_flat[mask]\nfiltered_labels = all_labels_flat[mask]\n\nprint(\"Accuracy:\", accuracy_score(filtered_labels, filtered_preds))\n\nprecision = precision_score(filtered_labels, filtered_preds, average=None, zero_division=0)\nrecall = recall_score(filtered_labels, filtered_preds, average=None, zero_division=0)\nf1 = f1_score(filtered_labels, filtered_preds, average=None, zero_division=0)\n\nfor i, (p, r, f) in enumerate(zip(precision, recall, f1)):\n    print(f\"Class {i} - Precision: {p}, Recall: {r}, F1 score: {f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader):\n        pixel_values = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(pixel_values=pixel_values, labels=labels)\n        loss, logits = outputs.loss, outputs.logits\n        upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n        predicted = upsampled_logits.argmax(dim=1)\n\n        # Set predictions to 255 wherever the ignore class is present in the labels\n        predicted[labels == 255] = 255\n\n        all_preds.append(predicted.detach().cpu())\n        all_labels.append(labels.detach().cpu())\n\nall_preds = torch.cat(all_preds)\nall_labels = torch.cat(all_labels)\n\n# Flatten the tensors\nall_preds_flat = all_preds.flatten()\nall_labels_flat = all_labels.flatten()\n\n# Mask to ignore the ignore_index\nignore_index = 255\nmask = all_labels_flat != ignore_index\n\n# Filter out the ignore_index\nfiltered_preds = all_preds_flat[mask]\nfiltered_labels = all_labels_flat[mask]\n\n# Define the colors for each specific value\ncolors = ['brown', 'green', 'black']\n\n# Create a colormap\ncmap = ListedColormap(colors)\n\n# Define the boundaries for the values\nboundaries = [0, 1, 2, 3]  # 0-1 -> black, 1-2 -> brown, 2-3 -> green\n\n# Create a normalization\nnorm = BoundaryNorm(boundaries, cmap.N, clip=True)\n\n# Plot a test image mask and the model's prediction\nidx = 4\nrows = 3\n\nfig, ax = plt.subplots(rows, 2, figsize=(10, 10))\n\nax[0][0].set_title(\"Test Image Mask\")\nax[0][1].set_title(\"Model Prediction\")\nfor i in range(rows):\n    ax[i][0].imshow(all_labels[idx * rows + i], cmap=cmap, norm=norm)\n    ax[i][1].imshow(all_preds[idx * rows + i], cmap=cmap, norm=norm)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_preds[idx]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list = os.listdir('Model')\n\n# Get the last number of the model\nmodel_list = sorted(model_list, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n\n# Save the model with the next number\n\nMODEL_SAVE_PATH = f\"Model/model_{int(model_list[-1].split('_')[-1].split('.')[0]) + 1}.pth\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load model\n\nmodel_num = 1\n\nmodel = SegformerForSemanticSegmentation.from_pretrained(f\"Model/model_{model_num}.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}